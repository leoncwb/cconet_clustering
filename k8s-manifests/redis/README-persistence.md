# Redis Cluster â€“ PersistÃªncia de Dados no Kubernetes

Por padrÃ£o, os manifests anteriores usavam `emptyDir`, o que significa que os dados eram **apagados** sempre que o Pod era reiniciado.  
Este documento descreve como adicionar **PersistentVolumes (PV)** e **PersistentVolumeClaims (PVC)** para garantir persistÃªncia dos dados no Redis Master e Slaves.

---

## ğŸ“‚ Estrutura de Arquivos



k8s-manifests/redis/
â”œâ”€â”€ redis-master-pv.yaml
â”œâ”€â”€ redis-master.yaml
â”œâ”€â”€ redis-slave1-pv.yaml
â”œâ”€â”€ redis-slave1.yaml
â”œâ”€â”€ redis-slave2-pv.yaml
â””â”€â”€ redis-slave2.yaml


---

## ğŸš€ Passos para Configurar

### 1. Criar os PersistentVolumes e PVCs

Cada nÃ³ terÃ¡ seu diretÃ³rio dedicado (`hostPath`):
- Master â†’ `/data/redis/master`
- Slave1 â†’ `/data/redis/slave1`
- Slave2 â†’ `/data/redis/slave2`

Aplicar os manifests:

```bash
kubectl apply -f redis-master-pv.yaml
kubectl apply -f redis-slave1-pv.yaml
kubectl apply -f redis-slave2-pv.yaml


Verificar:

kubectl get pv -n redis
kubectl get pvc -n redis

2. Atualizar os Deployments

Os deployments foram alterados para usar:

volumes:
- name: redis-master-data
  persistentVolumeClaim:
    claimName: redis-master-pvc


E cada container monta em /data:

volumeMounts:
- mountPath: /data
  name: redis-master-data


Isso garante que os dados sejam gravados no disco persistente.

3. Aplicar os Deployments

Atualizar os deployments do Master e Slaves:

kubectl apply -f redis-master.yaml
kubectl apply -f redis-slave1.yaml
kubectl apply -f redis-slave2.yaml

4. Validar PersistÃªncia

Entrar no Master e gravar uma chave:

kubectl exec -it -n redis deploy/redis-master -- \
  redis-cli SET teste "persistencia-ok"


Ler a chave de um Slave:

kubectl exec -it -n redis deploy/redis-slave1 -- \
  redis-cli GET teste


Deletar o Pod Master:

kubectl delete pod -n redis -l app=redis-master


Quando o Pod voltar, verificar se a chave ainda existe:

kubectl exec -it -n redis deploy/redis-master -- \
  redis-cli GET teste

ğŸ“Š Resultado Esperado

Os Pods podem ser reiniciados sem perda de dados.

O cluster mantÃ©m consistÃªncia mesmo apÃ³s falhas.

Cada Slave continua replicando do Master.
---
